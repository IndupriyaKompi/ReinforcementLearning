{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                        Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABSTRACT  ðŸ“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taxi-v3 is the environment that I have chosen to test the Reinforcement Learning models. Here, the agent is the taxi and it has to ensure to\n",
    "1. Drop off the passanger to the destination\n",
    "2. Take the shortest distance and save time\n",
    "3. Ensure passenger safety and follow traffic rules\n",
    "\n",
    "In Taxi-v3 environment, the agent starts at a random location and \n",
    "One **episode** is defined as one sequence of states, actions, and rewards that end with a terminal state\n",
    "\n",
    "\n",
    "Passenger locations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    - 4: in taxi\n",
    "    \n",
    "    Destinations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "        \n",
    "    Actions:\n",
    "    There are 6 discrete deterministic actions:\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east \n",
    "    - 3: move west \n",
    "    - 4: pickup passenger\n",
    "    - 5: dropoff passenger\n",
    "    \n",
    "    Rewards: \n",
    "    There is a reward of -1 for each action and an additional reward of +20 for delivering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
    "    \n",
    "    Rendering:\n",
    "    - blue: passenger\n",
    "    - magenta: destination\n",
    "    - yellow: empty taxi\n",
    "    - green: full taxi\n",
    "    - other letters (R, G, Y and B): locations for passengers and destinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gym is a toolkit for developing and comparing RL algorithms. The gym library is a collection of test problems â€” environments â€” that can be used to work out reinforcement learning algorithms. These environments have a shared interface, allowing one to write general algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learing | OpenAI | Taxi-V3 ðŸš•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the requried libraries\n",
    "import numpy as np \n",
    "import gym            # for Taxi environment\n",
    "import random         # To generate random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment using OpenAI \n",
    "\n",
    "Here we create Taxi-v3 environment and use it to train our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "env.render()             # renders environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Qtable \n",
    "**Actions**(Columns) and **States**(Rows) of a Qtable can be determined using openAI gym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size  6\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "print(\"Action size \", action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size  500\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.n\n",
    "print(\"State size \", state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means, we've 6 possible actions and 500 possible states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qtable consists of actions and states. At first, all the values in qtable is initialzed to zero as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Qtable to zeroes  ðŸ†’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. With the baseline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 5000\n",
    "total_test_episodes = 100\n",
    "max_steps = 99 \n",
    "alpha= 0.7 # Learning rate \n",
    "gamma = 0.8 # Discounting rate \n",
    "epsilon = 1.0 # Exploration rate\n",
    "min_epsilon = 0.01\n",
    "max_epsilon = 1.0\n",
    "decay_rate = 0.01 # Exponential decay rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are in the state of exploration, the action is chosen randomly. In the case of exploitation, we'll take the action with the highest Q-value for that state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Q Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        exp_exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * \n",
    "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "    \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "Score 7\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "Score 7\n",
      "****************************************************\n",
      "EPISODE  5\n",
      "Score 3\n",
      "****************************************************\n",
      "EPISODE  6\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  7\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  8\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  9\n",
      "Score 10\n",
      "****************************************************\n",
      "EPISODE  10\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  11\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  12\n",
      "Score 7\n",
      "****************************************************\n",
      "EPISODE  13\n",
      "Score 7\n",
      "****************************************************\n",
      "EPISODE  14\n",
      "Score 13\n",
      "****************************************************\n",
      "EPISODE  15\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  16\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  17\n",
      "Score 10\n",
      "****************************************************\n",
      "EPISODE  18\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  19\n",
      "Score 5\n",
      "****************************************************\n",
      "EPISODE  20\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  21\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  22\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  23\n",
      "Score 7\n",
      "****************************************************\n",
      "EPISODE  24\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  25\n",
      "Score 5\n",
      "****************************************************\n",
      "EPISODE  26\n",
      "Score 5\n",
      "****************************************************\n",
      "EPISODE  27\n",
      "Score 5\n",
      "****************************************************\n",
      "EPISODE  28\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  29\n",
      "Score 10\n",
      "****************************************************\n",
      "EPISODE  30\n",
      "Score 4\n",
      "****************************************************\n",
      "EPISODE  31\n",
      "Score 7\n",
      "****************************************************\n",
      "EPISODE  32\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  33\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  34\n",
      "Score 10\n",
      "****************************************************\n",
      "EPISODE  35\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  36\n",
      "Score 10\n",
      "****************************************************\n",
      "EPISODE  37\n",
      "Score 4\n",
      "****************************************************\n",
      "EPISODE  38\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  39\n",
      "Score 4\n",
      "****************************************************\n",
      "EPISODE  40\n",
      "Score 7\n",
      "****************************************************\n",
      "EPISODE  41\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  42\n",
      "Score 10\n",
      "****************************************************\n",
      "EPISODE  43\n",
      "Score 4\n",
      "****************************************************\n",
      "EPISODE  44\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  45\n",
      "Score 11\n",
      "****************************************************\n",
      "EPISODE  46\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  47\n",
      "Score 5\n",
      "****************************************************\n",
      "EPISODE  48\n",
      "Score 7\n",
      "****************************************************\n",
      "EPISODE  49\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  50\n",
      "Score 10\n",
      "****************************************************\n",
      "EPISODE  51\n",
      "Score 7\n",
      "****************************************************\n",
      "EPISODE  52\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  53\n",
      "Score 7\n",
      "****************************************************\n",
      "EPISODE  54\n",
      "Score 4\n",
      "****************************************************\n",
      "EPISODE  55\n",
      "Score 5\n",
      "****************************************************\n",
      "EPISODE  56\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  57\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  58\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  59\n",
      "Score 10\n",
      "****************************************************\n",
      "EPISODE  60\n",
      "Score 15\n",
      "****************************************************\n",
      "EPISODE  61\n",
      "Score 4\n",
      "****************************************************\n",
      "EPISODE  62\n",
      "Score 4\n",
      "****************************************************\n",
      "EPISODE  63\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  64\n",
      "Score 11\n",
      "****************************************************\n",
      "EPISODE  65\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  66\n",
      "Score 3\n",
      "****************************************************\n",
      "EPISODE  67\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  68\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  69\n",
      "Score 4\n",
      "****************************************************\n",
      "EPISODE  70\n",
      "Score 5\n",
      "****************************************************\n",
      "EPISODE  71\n",
      "Score 7\n",
      "****************************************************\n",
      "EPISODE  72\n",
      "Score 4\n",
      "****************************************************\n",
      "EPISODE  73\n",
      "Score 11\n",
      "****************************************************\n",
      "EPISODE  74\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  75\n",
      "Score 4\n",
      "****************************************************\n",
      "EPISODE  76\n",
      "Score 7\n",
      "****************************************************\n",
      "EPISODE  77\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  78\n",
      "Score 4\n",
      "****************************************************\n",
      "EPISODE  79\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  80\n",
      "Score 10\n",
      "****************************************************\n",
      "EPISODE  81\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  82\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  83\n",
      "Score 6\n",
      "****************************************************\n",
      "EPISODE  84\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  85\n",
      "Score 9\n",
      "****************************************************\n",
      "EPISODE  86\n",
      "Score 10\n",
      "****************************************************\n",
      "EPISODE  87\n",
      "Score 4\n",
      "****************************************************\n",
      "EPISODE  88\n",
      "Score 7\n",
      "****************************************************\n",
      "EPISODE  89\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  90\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  91\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  92\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  93\n",
      "Score 13\n",
      "****************************************************\n",
      "EPISODE  94\n",
      "Score 5\n",
      "****************************************************\n",
      "EPISODE  95\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  96\n",
      "Score 3\n",
      "****************************************************\n",
      "EPISODE  97\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  98\n",
      "Score 8\n",
      "****************************************************\n",
      "EPISODE  99\n",
      "Score 11\n",
      "Score over time: 7.32\n"
     ]
    }
   ],
   "source": [
    "env.reset()               # resets the environment and returns a random initial state\n",
    "rewards = []              \n",
    "\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # UNCOMMENT IT IF YOU WANT TO SEE OUR AGENT PLAYING\n",
    "        # env.render()\n",
    "        #Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            print (\"Score\", total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thus, the score over time with the baseline parameter is 7.32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our next goal is to improve the result by tuning the hyperparameters  ðŸŽ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try tuning hyperparameters to improve the acquried results ðŸ› "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [0.6,0.65,0.7,0.75,0.8,0.9]\n",
    "gamma_list = [0.5,0.54,0.618,0.71,0.78,0.88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha : 0.6\n",
      "gamma : 0.5\n",
      "Score over time: 8.09\n",
      "Alpha : 0.6\n",
      "gamma : 0.54\n",
      "Score over time: 8.02\n",
      "Alpha : 0.6\n",
      "gamma : 0.618\n",
      "Score over time: 8.09\n",
      "Alpha : 0.6\n",
      "gamma : 0.71\n",
      "Score over time: 7.79\n",
      "Alpha : 0.6\n",
      "gamma : 0.78\n",
      "Score over time: 8.24\n",
      "Alpha : 0.6\n",
      "gamma : 0.88\n",
      "Score over time: 8.16\n",
      "Alpha : 0.65\n",
      "gamma : 0.5\n",
      "Score over time: 7.99\n",
      "Alpha : 0.65\n",
      "gamma : 0.54\n",
      "Score over time: 7.75\n",
      "Alpha : 0.65\n",
      "gamma : 0.618\n",
      "Score over time: 7.71\n",
      "Alpha : 0.65\n",
      "gamma : 0.71\n",
      "Score over time: 7.81\n",
      "Alpha : 0.65\n",
      "gamma : 0.78\n",
      "Score over time: 7.58\n",
      "Alpha : 0.65\n",
      "gamma : 0.88\n",
      "Score over time: 7.53\n",
      "Alpha : 0.7\n",
      "gamma : 0.5\n",
      "Score over time: 7.76\n",
      "Alpha : 0.7\n",
      "gamma : 0.54\n",
      "Score over time: 7.68\n",
      "Alpha : 0.7\n",
      "gamma : 0.618\n",
      "Score over time: 7.73\n",
      "Alpha : 0.7\n",
      "gamma : 0.71\n",
      "Score over time: 7.97\n",
      "Alpha : 0.7\n",
      "gamma : 0.78\n",
      "Score over time: 7.66\n",
      "Alpha : 0.7\n",
      "gamma : 0.88\n",
      "Score over time: 8.08\n",
      "Alpha : 0.75\n",
      "gamma : 0.5\n",
      "Score over time: 7.9\n",
      "Alpha : 0.75\n",
      "gamma : 0.54\n",
      "Score over time: 7.61\n",
      "Alpha : 0.75\n",
      "gamma : 0.618\n",
      "Score over time: 8.22\n",
      "Alpha : 0.75\n",
      "gamma : 0.71\n",
      "Score over time: 8.16\n",
      "Alpha : 0.75\n",
      "gamma : 0.78\n",
      "Score over time: 8.2\n",
      "Alpha : 0.75\n",
      "gamma : 0.88\n",
      "Score over time: 7.69\n",
      "Alpha : 0.8\n",
      "gamma : 0.5\n",
      "Score over time: 7.68\n",
      "Alpha : 0.8\n",
      "gamma : 0.54\n",
      "Score over time: 7.33\n",
      "Alpha : 0.8\n",
      "gamma : 0.618\n",
      "Score over time: 7.97\n",
      "Alpha : 0.8\n",
      "gamma : 0.71\n",
      "Score over time: 7.36\n",
      "Alpha : 0.8\n",
      "gamma : 0.78\n",
      "Score over time: 7.43\n",
      "Alpha : 0.8\n",
      "gamma : 0.88\n",
      "Score over time: 7.65\n",
      "Alpha : 0.9\n",
      "gamma : 0.5\n",
      "Score over time: 7.91\n",
      "Alpha : 0.9\n",
      "gamma : 0.54\n",
      "Score over time: 7.71\n",
      "Alpha : 0.9\n",
      "gamma : 0.618\n",
      "Score over time: 7.88\n",
      "Alpha : 0.9\n",
      "gamma : 0.71\n",
      "Score over time: 7.59\n",
      "Alpha : 0.9\n",
      "gamma : 0.78\n",
      "Score over time: 7.73\n",
      "Alpha : 0.9\n",
      "gamma : 0.88\n",
      "Score over time: 8.31\n"
     ]
    }
   ],
   "source": [
    "for lr in alpha_list: \n",
    "    alpha = lr # Learning rate \n",
    "    for g in gamma_list:\n",
    "        qtable = np.zeros((state_size, action_size))\n",
    "        total_episodes = 5000\n",
    "        total_test_episodes = 100\n",
    "        max_steps = 99 \n",
    "        gamma = g # Discounting rate \n",
    "        epsilon = 1.0 # Exploration rate\n",
    "        min_epsilon = 0.01\n",
    "        max_epsilon = 1.0\n",
    "        decay_rate = 0.01 # Exponential decay rate\n",
    "        \n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            # Reset the environment\n",
    "            state = env.reset()\n",
    "            step = 0\n",
    "            done = False\n",
    "            for step in range(max_steps):\n",
    "                exp_exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "                if exp_exp_tradeoff > epsilon:\n",
    "                    action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        # Else doing a random choice --> exploration\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "        \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "                qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * \n",
    "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                \n",
    "        # Our new state is state\n",
    "                state = new_state\n",
    "        \n",
    "        # If done : finish episode\n",
    "                if done == True: \n",
    "                    break\n",
    "        env.reset()               # resets the environment and returns a random initial state\n",
    "\n",
    "        rewards = []              \n",
    "\n",
    "        for episode in range(total_test_episodes):\n",
    "            state = env.reset()\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "            #print(\"****************************************************\")\n",
    "            #print(\"EPISODE \", episode)\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                # UNCOMMENT IT IF YOU WANT TO SEE OUR AGENT PLAYING\n",
    "                #env.render()\n",
    "                #Take the action (index) that have the maximum expected future reward given that state\n",
    "                action = np.argmax(qtable[state,:])\n",
    "\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "\n",
    "                total_rewards += reward\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(total_rewards)\n",
    "                    #print (\"Score\", total_rewards)\n",
    "                    break\n",
    "                state = new_state\n",
    "        env.close()\n",
    "        print(\"Alpha :\",alpha)\n",
    "        print(\"gamma :\",g)\n",
    "    \n",
    "        print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thus, the optimum value of alpha and gamma is 0.9 and 0.88 respectively for which the score over time is 8.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_list = [0.01,0.1,0.2,0.8,1.0]\n",
    "decay_rate_list = [0.01,0.1,0.2,0.6,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploration Rate : 0.01\n",
      "Decay Rate : 0.01\n",
      "Score over time: 7.67\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.01\n",
      "Decay Rate : 0.1\n",
      "Score over time: 7.82\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.01\n",
      "Decay Rate : 0.2\n",
      "Score over time: 7.54\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.01\n",
      "Decay Rate : 0.6\n",
      "Score over time: 8.32\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.01\n",
      "Decay Rate : 1\n",
      "Score over time: 7.53\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.1\n",
      "Decay Rate : 0.01\n",
      "Score over time: 7.45\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.1\n",
      "Decay Rate : 0.1\n",
      "Score over time: 7.83\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.1\n",
      "Decay Rate : 0.2\n",
      "Score over time: 8.13\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.1\n",
      "Decay Rate : 0.6\n",
      "Score over time: 8.2\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.1\n",
      "Decay Rate : 1\n",
      "Score over time: 7.53\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.2\n",
      "Decay Rate : 0.01\n",
      "Score over time: 7.61\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.2\n",
      "Decay Rate : 0.1\n",
      "Score over time: 8.3\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.2\n",
      "Decay Rate : 0.2\n",
      "Score over time: 8.41\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.2\n",
      "Decay Rate : 0.6\n",
      "Score over time: 7.71\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.2\n",
      "Decay Rate : 1\n",
      "Score over time: 7.73\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.8\n",
      "Decay Rate : 0.01\n",
      "Score over time: 8.13\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.8\n",
      "Decay Rate : 0.1\n",
      "Score over time: 8.36\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.8\n",
      "Decay Rate : 0.2\n",
      "Score over time: 8.51\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.8\n",
      "Decay Rate : 0.6\n",
      "Score over time: 7.69\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 0.8\n",
      "Decay Rate : 1\n",
      "Score over time: 8.06\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 1.0\n",
      "Decay Rate : 0.01\n",
      "Score over time: 7.64\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 1.0\n",
      "Decay Rate : 0.1\n",
      "Score over time: 8.03\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 1.0\n",
      "Decay Rate : 0.2\n",
      "Score over time: 8.23\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 1.0\n",
      "Decay Rate : 0.6\n",
      "Score over time: 8.19\n",
      "----------------------------------------------------------\n",
      "Exploration Rate : 1.0\n",
      "Decay Rate : 1\n",
      "Score over time: 8.47\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for ep in epsilon_list: \n",
    "    epsilon = ep # Exploration rate \n",
    "    for dr in decay_rate_list:\n",
    "        qtable = np.zeros((state_size, action_size))\n",
    "        total_episodes = 5000\n",
    "        total_test_episodes = 100\n",
    "        max_steps = 99\n",
    "        alpha = 0.9  # Learning rate\n",
    "        gamma = 0.88 # Discounting rate \n",
    "        min_epsilon = 0.01\n",
    "        max_epsilon = 1.0\n",
    "        decay_rate = dr # Exponential decay rate\n",
    "        #print(decay_rate)\n",
    "        \n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            # Reset the environment\n",
    "            state = env.reset()\n",
    "            step = 0\n",
    "            done = False\n",
    "            for step in range(max_steps):\n",
    "                exp_exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "                if exp_exp_tradeoff > epsilon:\n",
    "                    action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        # Else doing a random choice --> exploration\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "        \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "                qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * \n",
    "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                \n",
    "        # Our new state is state\n",
    "                state = new_state\n",
    "        \n",
    "        # If done : finish episode\n",
    "                if done == True: \n",
    "                    break\n",
    "        env.reset()               # resets the environment and returns a random initial state\n",
    "\n",
    "        rewards = []              \n",
    "\n",
    "        for episode in range(total_test_episodes):\n",
    "            state = env.reset()\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "            #print(\"****************************************************\")\n",
    "            #print(\"EPISODE \", episode)\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                # UNCOMMENT IT IF YOU WANT TO SEE OUR AGENT PLAYING\n",
    "                #env.render()\n",
    "                #Take the action (index) that have the maximum expected future reward given that state\n",
    "                action = np.argmax(qtable[state,:])\n",
    "\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "\n",
    "                total_rewards += reward\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(total_rewards)\n",
    "                    #print (\"Score\", total_rewards)\n",
    "                    break\n",
    "                state = new_state\n",
    "        env.close()\n",
    "        print(\"Exploration Rate :\",epsilon)\n",
    "        print(\"Decay Rate :\",dr)\n",
    "    \n",
    "        print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))\n",
    "        print (\"----------------------------------------------------------\")\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thus, the optimum value of Exploration Rate  and Decay Rate is 0.8 and 0.2 respectively for which the score over time is 8.51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So far, the % increase from the baseline performance is ((7.32-8.51)/7.32)*100 that equals : 16.25%   ðŸ˜²\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now tune the number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes_list = [5000,10000,25000,28000,50000,70000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episodes : 5000\n",
      "Score over time: 7.86\n",
      "----------------------------------------------------------\n",
      "Total Episodes : 10000\n",
      "Score over time: 7.94\n",
      "----------------------------------------------------------\n",
      "Total Episodes : 25000\n",
      "Score over time: 7.85\n",
      "----------------------------------------------------------\n",
      "Total Episodes : 28000\n",
      "Score over time: 7.68\n",
      "----------------------------------------------------------\n",
      "Total Episodes : 50000\n",
      "Score over time: 7.46\n",
      "----------------------------------------------------------\n",
      "Total Episodes : 70000\n",
      "Score over time: 8.25\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "    for tel in total_episodes_list:\n",
    "        total_episodes = tel\n",
    "        #print(dr)\n",
    "        qtable = np.zeros((state_size, action_size))\n",
    "        \n",
    "        #total_episodes = 5000\n",
    "        total_test_episodes = 100\n",
    "        max_steps = 99\n",
    "        \n",
    "        alpha = 0.9\n",
    "        gamma = 0.88 # Discounting rate \n",
    "        \n",
    "        epsilon = 0.8 # Exploration rate\n",
    "        min_epsilon = 0.01\n",
    "        max_epsilon = 1.0\n",
    "        decay_rate = 0.2 # Exponential decay rate\n",
    "        #print(decay_rate)\n",
    "        \n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            # Reset the environment\n",
    "            state = env.reset()\n",
    "            step = 0\n",
    "            done = False\n",
    "            for step in range(max_steps):\n",
    "                exp_exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "                if exp_exp_tradeoff > epsilon:\n",
    "                    action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        # Else doing a random choice --> exploration\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "        \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "                qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * \n",
    "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "                \n",
    "        # Our new state is state\n",
    "                state = new_state\n",
    "        \n",
    "        # If done : finish episode\n",
    "                if done == True: \n",
    "                    break\n",
    "        env.reset()               # resets the environment and returns a random initial state\n",
    "\n",
    "        rewards = []              \n",
    "\n",
    "        for episode in range(total_test_episodes):\n",
    "            state = env.reset()\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "            #print(\"****************************************************\")\n",
    "            #print(\"EPISODE \", episode)\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                # UNCOMMENT IT IF YOU WANT TO SEE OUR AGENT PLAYING\n",
    "                #env.render()\n",
    "                #Take the action (index) that have the maximum expected future reward given that state\n",
    "                action = np.argmax(qtable[state,:])\n",
    "\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "\n",
    "                total_rewards += reward\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(total_rewards)\n",
    "                    #print (\"Score\", total_rewards)\n",
    "                    break\n",
    "                state = new_state\n",
    "        env.close()\n",
    "        print(\"Total Episodes :\",total_episodes)\n",
    "        #print(\"Decay Rate :\",dr)\n",
    "    \n",
    "        print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))\n",
    "        print (\"----------------------------------------------------------\")\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The score over time for 70000 number of episodes is 8.25\n",
    "\n",
    "## Resulting in the % increase from the baseline of 12.70%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to try for policy other than argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 5000\n",
    "total_test_episodes = 100\n",
    "max_steps = 99 \n",
    "alpha= 0.9 # Learning rate \n",
    "gamma = 0.88 # Discounting rate \n",
    "epsilon = 0.8 # Exploration rate\n",
    "min_epsilon = 0.01\n",
    "max_epsilon = 1.0\n",
    "decay_rate = 0.2 # Exponential decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        exp_exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * \n",
    "                                    np.min(qtable[new_state, :]) - qtable[state, action])\n",
    "                \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "    \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 4.37\n"
     ]
    }
   ],
   "source": [
    "env.reset()               # resets the environment and returns a random initial state\n",
    "rewards = []              \n",
    "\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    #print(\"****************************************************\")\n",
    "    #print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # UNCOMMENT IT IF YOU WANT TO SEE OUR AGENT PLAYING\n",
    "        # env.render()\n",
    "        #Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            #print (\"Score\", total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this notebook, we can conclude, that the score over time of an algorithm, in an environment can be imrpoved by tuning the hyperparameters. Trying out through the various values of hyperparameter and iterating them is one of the ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author :\n",
    "\n",
    "Kompi Sadasivappa, Indupriya\n",
    "\n",
    "MS Information Systems, \n",
    "\n",
    "Northeastern University, Boston\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/simoninithomas/Deep_reinforcement_learning_Course\n",
    "\n",
    "https://www.youtube.com/results?search_query=Q-learning+with+numpy+and+OpenAI+Taxi-v2\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLQLZ37V8CnUTdIoJJdvmmFoQJntZ9dp5Q\n",
    "\n",
    "https://github.com/Lodur03/Q-Learning-Taxi-v2/blob/master/Q-Learning-Taxi.ipynb\n",
    "\n",
    "https://www.biostat.wisc.edu/~craven/cs760/lectures/reinforcement.pdf\n",
    "\n",
    "https://towardsdatascience.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Licensing\n",
    "\n",
    "Copyright 2020 Indupriya Kompi Sadasivappa\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
